{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from src.util import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyspark.sql.functions as F\n",
    "import s3fs\n",
    "# import nltk\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from pyspark.ml.feature import CountVectorizer, IDF\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql import Row\n",
    "# PUNCTUATION = set(string.punctuation)\n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "# from sklearn import preprocessing\n",
    "# import codecs\n",
    "# import string\n",
    "# import re\n",
    "# from pyspark.sql.functions import isnan, when, count, col\n",
    "# import spacy\n",
    "# from spacy.lang.en import English\n",
    "# from spacy import displacy\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "# import logging\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\n",
    " 's3://aws-logs-816063959671-us-east-1/data/tldr-training-data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = df.sample(withReplacement=False, fraction=0.1)\n",
    "subset.cache()\n",
    "subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "import pickle\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "addl_punctuation = set(['...', '`', '¿','⸮', '``', \"''\"])\n",
    "PUNCTUATION = PUNCTUATION.union(addl_punctuation)\n",
    "\n",
    "CONTRACTIONS = {\n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\"i'd\": \"i would\",\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    text = text.lower()\n",
    "\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = [CONTRACTIONS[w] if w in CONTRACTIONS else w for w in text]\n",
    "\n",
    "        text = \" \".join(new_text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    if remove_stopwords==True:\n",
    "        tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def word_length(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    return len(tokens)\n",
    "\n",
    "def clean_data(df, n_words_summary=50, remove_stopwords=True):\n",
    "    # Get rid of all rows where subreddit is null (these are spam)\n",
    "    df = df.filter(df.subreddit.isNotNull())\n",
    "    # Lowercase columns:\n",
    "    for col in ['body','content','normalizedBody','subreddit','summary','title']:\n",
    "        df = df.withColumn(col, F.lower(F.col(col)))\n",
    "    # Converts 'null' strings in the title column back to null values\n",
    "    df = df.withColumn('title', when(df.title == 'null', F.lit(None)).otherwise(df.title))\n",
    "    \n",
    "\n",
    "    # Creat edit(bool) and edit_len columns, while removing 'edit:%' from summary column\n",
    "    split_col = F.split(df['summary'], '(edit:|[^a-z]edit)')\n",
    "    df = df.withColumn('edit', split_col.getItem(1))\n",
    "    df = df.withColumn('summary', split_col.getItem(0))\n",
    "    function = udf(word_length, LongType())\n",
    "    df = df.withColumn('summary_len', function(df.summary))\n",
    "        # Creates edit_len column, number of words from 'edit'\n",
    "    df = df.withColumn('edit', df.edit).na.fill('')\n",
    "    df = df.withColumn('edit_len', function(df.edit))\n",
    "        # Converts -1 in edit_len column to null\n",
    "    df = df.withColumn('edit_len',\n",
    "        when(df.edit_len == -1, F.lit(0)).otherwise(df.edit_len))\n",
    "    df = df.withColumn('edit', when(df.edit.isNull(), F.lit(0)).otherwise(1))\n",
    "    # Remove all rows where summary contains less than 5 words\n",
    "    df = df.filter(df.summary_len >= 5)\n",
    "    # Remove all rows where summary contains greater than n_words_summary words\n",
    "    df = df.filter((df.summary_len <= n_words_summary))\n",
    "    # Remove all rows where the summary length is not less than 50% of the content length\n",
    "    df = df.filter(df.summary_len <= df.content_len*0.5)\n",
    "    # Clean Content column\n",
    "    cleantext_udf = udf(clean_text, StringType())\n",
    "    df = df.withColumn('content', cleantext_udf(df.content, F.lit(remove_stopwords)))\n",
    "    df = df.withColumn('summary', cleantext_udf(df.summary, F.lit(False)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = clean_data(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267460"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = newdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.drop(['body', 'normalizedBody', 'author', 'id', 'subreddit', 'subreddit_id',\n",
    "         'title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_len</th>\n",
       "      <th>edit</th>\n",
       "      <th>edit_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mechwarrior online begun use mouse turret aspe...</td>\n",
       "      <td>213</td>\n",
       "      <td>yes joysticks in modern games have apparently ...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>used work mgm studios known disney 's hollywoo...</td>\n",
       "      <td>124</td>\n",
       "      <td>i knocked susan lucci on her ass</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good audio/video quality including mics banter...</td>\n",
       "      <td>101</td>\n",
       "      <td>good job but more firefall talk</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kids little 20 's always told look one another...</td>\n",
       "      <td>246</td>\n",
       "      <td>4th grader beat an 8th grade bully to bloody t...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think like mailing pages book different people...</td>\n",
       "      <td>166</td>\n",
       "      <td>always look for the highest seeded torrents an...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  content_len  \\\n",
       "0  mechwarrior online begun use mouse turret aspe...          213   \n",
       "1  used work mgm studios known disney 's hollywoo...          124   \n",
       "2  good audio/video quality including mics banter...          101   \n",
       "3  kids little 20 's always told look one another...          246   \n",
       "4  think like mailing pages book different people...          166   \n",
       "\n",
       "                                             summary  summary_len  edit  \\\n",
       "0  yes joysticks in modern games have apparently ...           19     1   \n",
       "1                   i knocked susan lucci on her ass            7     1   \n",
       "2                    good job but more firefall talk            6     1   \n",
       "3  4th grader beat an 8th grade bully to bloody t...           10     1   \n",
       "4  always look for the highest seeded torrents an...           10     1   \n",
       "\n",
       "   edit_len  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.replace('', np.NaN)\n",
    "pdf = pdf.replace(float('nan'), np.NaN)\n",
    "pdf.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents=ascii)\n",
    "cv.fit_transform(pdf['content'])\n",
    "vocab = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299734"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert glove vectors to pkl (which was later cp'd to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = s3fs.S3FileSystem(anon=False)\n",
    "# embeddings_index = {}\n",
    "# with s3.open(\n",
    "#     's3://aws-logs-816063959671-us-east-1/data/glove.42B.300d.txt',\n",
    "#         'rb') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0] \n",
    "#         embedding = np.asarray(values[1:], dtype='float32') \n",
    "#         embeddings_index[word.decode('utf-8')] = embedding \n",
    "\n",
    "# with open('../data/glove_embeddings.pickle', 'wb') as handle:\n",
    "#     pickle.dump(\n",
    "#         embeddings_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "          \n",
    "# print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "with s3.open(\n",
    "    's3://aws-logs-816063959671-us-east-1/data/glove_embeddings.pickle', \n",
    "    'rb') as handle:\n",
    "    glove_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 101320\n",
      "Percent of words that are missing from vocabulary: 33.800000000000004%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in vocab.items():\n",
    "    if word not in glove_embeddings:\n",
    "        missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(vocab),4)*100\n",
    "            \n",
    "print(\"Number of words missing from glove embeddings:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 299734\n",
      "Number of words we will use: 198418\n",
      "Percent of words we will use: 66.2%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, _ in vocab.items():\n",
    "    if word in glove_embeddings:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, idx in vocab_to_int.items():\n",
    "    int_to_vocab[idx] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(vocab),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(vocab))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198418\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match glove's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in glove_embeddings:\n",
    "        word_embedding_matrix[i] = glove_embeddings[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        glove_embeddings[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode text and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_len</th>\n",
       "      <th>edit</th>\n",
       "      <th>edit_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mechwarrior online begun use mouse turret aspe...</td>\n",
       "      <td>213</td>\n",
       "      <td>yes joysticks in modern games have apparently ...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>used work mgm studios known disney 's hollywoo...</td>\n",
       "      <td>124</td>\n",
       "      <td>i knocked susan lucci on her ass</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good audio/video quality including mics banter...</td>\n",
       "      <td>101</td>\n",
       "      <td>good job but more firefall talk</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kids little 20 's always told look one another...</td>\n",
       "      <td>246</td>\n",
       "      <td>4th grader beat an 8th grade bully to bloody t...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think like mailing pages book different people...</td>\n",
       "      <td>166</td>\n",
       "      <td>always look for the highest seeded torrents an...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  content_len  \\\n",
       "0  mechwarrior online begun use mouse turret aspe...          213   \n",
       "1  used work mgm studios known disney 's hollywoo...          124   \n",
       "2  good audio/video quality including mics banter...          101   \n",
       "3  kids little 20 's always told look one another...          246   \n",
       "4  think like mailing pages book different people...          166   \n",
       "\n",
       "                                             summary  summary_len  edit  \\\n",
       "0  yes joysticks in modern games have apparently ...           19     1   \n",
       "1                   i knocked susan lucci on her ass            7     1   \n",
       "2                    good job but more firefall talk            6     1   \n",
       "3  4th grader beat an 8th grade bully to bloody t...           10     1   \n",
       "4  always look for the highest seeded torrents an...           10     1   \n",
       "\n",
       "   edit_len  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(row, column, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    sentence_ints = []\n",
    "    for word in row[column].split():\n",
    "        if word in vocab_to_int:\n",
    "            sentence_ints.append(vocab_to_int[word])\n",
    "        else:\n",
    "            sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "    if eos:\n",
    "        sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "    return sentence_ints\n",
    "\n",
    "\n",
    "def unk_counter(row, column):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in row[column]:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_summary = pd.DataFrame(\n",
    "    pdf.apply(convert_to_ints, axis=1, args=('summary',True)))\n",
    "int_summary.columns = ['summary']\n",
    "unk_summary = pd.DataFrame(\n",
    "    int_summary.apply(unk_counter, axis=1, args=('summary',)))\n",
    "unk_summary.columns = ['unk_count_summary']\n",
    "\n",
    "int_content = pd.DataFrame(\n",
    "    pdf.apply(convert_to_ints, axis=1, args=('content',True)))\n",
    "int_content.columns = ['content']\n",
    "unk_content = pd.DataFrame(\n",
    "    int_content.apply(unk_counter, axis=1, args=('content',)))\n",
    "unk_content.columns = ['unk_count_content']\n",
    "\n",
    "dfs = [int_content, int_summaries, unk_content, unk_summary,\n",
    "      pdf['content_len'], pdf['summary_len']]\n",
    "\n",
    "encoded_df = pd.concat((dfs), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "      <th>unk_count_content</th>\n",
       "      <th>unk_count_summary</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 198414, 10, 11, 12, 1...</td>\n",
       "      <td>[2361, 23386, 2502, 25, 26, 1340, 1626, 1060, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 101, 102, 103, 104, 105, 198414, 106, 10...</td>\n",
       "      <td>[198414, 4412, 139, 140, 4229, 6700, 5133, 198...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[168, 198414, 171, 172, 173, 174, 175, 176, 17...</td>\n",
       "      <td>[168, 1790, 12028, 6625, 206, 762, 198416]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[215, 14, 216, 198414, 217, 218, 90, 219, 220,...</td>\n",
       "      <td>[227, 233, 254, 27677, 232, 225, 264, 4981, 25...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[34, 134, 300, 301, 302, 303, 304, 305, 306, 3...</td>\n",
       "      <td>[217, 90, 1067, 1068, 471, 23309, 33216, 2934,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 198414, 10, 11, 12, 1...   \n",
       "1  [100, 101, 102, 103, 104, 105, 198414, 106, 10...   \n",
       "2  [168, 198414, 171, 172, 173, 174, 175, 176, 17...   \n",
       "3  [215, 14, 216, 198414, 217, 218, 90, 219, 220,...   \n",
       "4  [34, 134, 300, 301, 302, 303, 304, 305, 306, 3...   \n",
       "\n",
       "                                             summary  unk_count_content  \\\n",
       "0  [2361, 23386, 2502, 25, 26, 1340, 1626, 1060, ...                  6   \n",
       "1  [198414, 4412, 139, 140, 4229, 6700, 5133, 198...                  3   \n",
       "2         [168, 1790, 12028, 6625, 206, 762, 198416]                  3   \n",
       "3  [227, 233, 254, 27677, 232, 225, 264, 4981, 25...                  2   \n",
       "4  [217, 90, 1067, 1068, 471, 23309, 33216, 2934,...                  9   \n",
       "\n",
       "   unk_count_summary  content_len  summary_len  \n",
       "0                  1          213           19  \n",
       "1                  1          124            7  \n",
       "2                  0          101            6  \n",
       "3                  0          246           10  \n",
       "4                  0          166           10  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unk_count_content</th>\n",
       "      <th>unk_count_summary</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>267459.000000</td>\n",
       "      <td>267459.000000</td>\n",
       "      <td>267459.000000</td>\n",
       "      <td>267459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.788999</td>\n",
       "      <td>1.473994</td>\n",
       "      <td>208.916701</td>\n",
       "      <td>20.689294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.298682</td>\n",
       "      <td>1.542533</td>\n",
       "      <td>80.302270</td>\n",
       "      <td>10.749811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>389.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unk_count_content  unk_count_summary    content_len    summary_len\n",
       "count      267459.000000      267459.000000  267459.000000  267459.000000\n",
       "mean            3.788999           1.473994     208.916701      20.689294\n",
       "std             5.298682           1.542533      80.302270      10.749811\n",
       "min             0.000000           0.000000     100.000000       5.000000\n",
       "25%             1.000000           0.000000     141.000000      12.000000\n",
       "50%             3.000000           1.000000     192.000000      19.000000\n",
       "75%             5.000000           2.000000     266.000000      27.000000\n",
       "max           389.000000          35.000000     400.000000      50.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f33aea700f0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unk[0].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\\\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75\n",
    "rate = 1-keep_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198418"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input params to encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n",
    "\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding / Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                        output_time_major=False,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return training_logits\n",
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                            output_time_major=False,\n",
    "                                            impute_finished=True,\n",
    "                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "    \n",
    "    \n",
    "    #initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encoded_df.sort_values(by='content_len', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = encoded_df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_short = test_df.summary\n",
    "content_short = test_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 37286)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda/lib/python3.7/socketserver.py\", line 313, in _handle_request_noblock\n",
      "  File \"/home/hadoop/anaconda/lib/python3.7/socketserver.py\", line 344, in process_request\n",
      "  File \"/home/hadoop/anaconda/lib/python3.7/socketserver.py\", line 357, in finish_request\n",
      "  File \"/home/hadoop/anaconda/lib/python3.7/socketserver.py\", line 717, in __init__\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 577, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,49,198419] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node decode/decoder/transpose (defined at <ipython-input-214-7cd1f6deff1f>:17) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'decode/decoder/transpose', defined at:\n  File \"/home/hadoop/anaconda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n  File \"/home/hadoop/anaconda/lib/python3.7/runpy.py\", line 85, in _run_code\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/events.py\", line 88, in _run\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n  File \"<ipython-input-218-2482e3ce02af>\", line 20, in <module>\n    batch_size)\n  File \"<ipython-input-215-605aa3e33839>\", line 26, in seq2seq_model\n    num_layers)\n  File \"<ipython-input-214-7cd1f6deff1f>\", line 75, in decoding_layer\n    max_summary_length)\n  File \"<ipython-input-214-7cd1f6deff1f>\", line 17, in training_decoding_layer\n    maximum_iterations=max_summary_length)\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 337, in dynamic_decode\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 381, in map_structure\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 381, in <listcomp>\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py\", line 64, in _transpose_batch_time\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1674, in transpose\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 10238, in transpose\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,49,198419] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node decode/decoder/transpose (defined at <ipython-input-214-7cd1f6deff1f>:17) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,49,198419] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node decode/decoder/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-3b1c3fd1d2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0msummary_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                  \u001b[0mtext_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                  keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,49,198419] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node decode/decoder/transpose (defined at <ipython-input-214-7cd1f6deff1f>:17) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'decode/decoder/transpose', defined at:\n  File \"/home/hadoop/anaconda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n  File \"/home/hadoop/anaconda/lib/python3.7/runpy.py\", line 85, in _run_code\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\n  File \"/home/hadoop/anaconda/lib/python3.7/asyncio/events.py\", line 88, in _run\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n  File \"<ipython-input-218-2482e3ce02af>\", line 20, in <module>\n    batch_size)\n  File \"<ipython-input-215-605aa3e33839>\", line 26, in seq2seq_model\n    num_layers)\n  File \"<ipython-input-214-7cd1f6deff1f>\", line 75, in decoding_layer\n    max_summary_length)\n  File \"<ipython-input-214-7cd1f6deff1f>\", line 17, in training_decoding_layer\n    maximum_iterations=max_summary_length)\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 337, in dynamic_decode\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 381, in map_structure\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 381, in <listcomp>\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py\", line 64, in _transpose_batch_time\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1674, in transpose\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 10238, in transpose\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n  File \"/home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,49,198419] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node decode/decoder/transpose (defined at <ipython-input-214-7cd1f6deff1f>:17) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(content_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(summary_short, content_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(content_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
