{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from src.util import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyspark.sql.functions as F\n",
    "import s3fs\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import collections\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\n",
    " 's3://aws-logs-816063959671-us-east-1/data/tldr-training-data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308249"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = df.sample(withReplacement=False, fraction=0.1)\n",
    "subset.cache()\n",
    "subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "import pickle\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "addl_punctuation = set(['...', '`', '¿','⸮', '``', \"''\"])\n",
    "PUNCTUATION = PUNCTUATION.union(addl_punctuation)\n",
    "\n",
    "CONTRACTIONS = {\n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\"i'd\": \"i would\",\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    text = text.lower()\n",
    "\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = [CONTRACTIONS[w] if w in CONTRACTIONS else w for w in text]\n",
    "\n",
    "        text = \" \".join(new_text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    if remove_stopwords==True:\n",
    "        tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def word_length(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    return len(tokens)\n",
    "\n",
    "def clean_data(df, n_words_summary=50, remove_stopwords=True):\n",
    "    # Get rid of all rows where subreddit is null (these are spam)\n",
    "    df = df.filter(df.subreddit.isNotNull())\n",
    "    # Lowercase columns:\n",
    "    for col in ['body','content','normalizedBody','subreddit','summary','title']:\n",
    "        df = df.withColumn(col, F.lower(F.col(col)))\n",
    "    # Converts 'null' strings in the title column back to null values\n",
    "    df = df.withColumn('title', when(df.title == 'null', F.lit(None)).otherwise(df.title))\n",
    "    \n",
    "\n",
    "    # Creat edit(bool) and edit_len columns, while removing 'edit:%' from summary column\n",
    "    split_col = F.split(df['summary'], '(edit:|[^a-z]edit)')\n",
    "    df = df.withColumn('edit', split_col.getItem(1))\n",
    "    df = df.withColumn('summary', split_col.getItem(0))\n",
    "    function = udf(word_length, LongType())\n",
    "    df = df.withColumn('summary_len', function(df.summary))\n",
    "        # Creates edit_len column, number of words from 'edit'\n",
    "    df = df.withColumn('edit', df.edit).na.fill('')\n",
    "    df = df.withColumn('edit_len', function(df.edit))\n",
    "        # Converts -1 in edit_len column to null\n",
    "    df = df.withColumn('edit_len',\n",
    "        when(df.edit_len == -1, F.lit(0)).otherwise(df.edit_len))\n",
    "    df = df.withColumn('edit', when(df.edit.isNull(), F.lit(0)).otherwise(1))\n",
    "    # Remove all rows where summary contains less than 5 words\n",
    "    df = df.filter(df.summary_len >= 5)\n",
    "    # Remove all rows where summary contains greater than n_words_summary words\n",
    "    df = df.filter((df.summary_len <= n_words_summary))\n",
    "    # Remove all rows where the summary length is not less than 50% of the content length\n",
    "    df = df.filter(df.summary_len <= df.content_len*0.5)\n",
    "    # Clean Content column\n",
    "    cleantext_udf = udf(clean_text, StringType())\n",
    "    df = df.withColumn('content', cleantext_udf(df.content, F.lit(remove_stopwords)))\n",
    "    df = df.withColumn('summary', cleantext_udf(df.summary, F.lit(False)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = clean_data(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267475"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_len</th>\n",
       "      <th>edit</th>\n",
       "      <th>edit_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>think fixed either utc standard utc+1 year aro...</td>\n",
       "      <td>178</td>\n",
       "      <td>shifting seasonal time is no longer worth it</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>back still listened skrillex songs almost alwa...</td>\n",
       "      <td>182</td>\n",
       "      <td>do not listen to dubstep right before you go t...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>definitive line comes hours anything someone c...</td>\n",
       "      <td>134</td>\n",
       "      <td>there is not a definitive skill level it is al...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good audio/video quality including mics banter...</td>\n",
       "      <td>101</td>\n",
       "      <td>good job but more firefall talk</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>narrative hl1 lead believe black mesa 's first...</td>\n",
       "      <td>104</td>\n",
       "      <td>the scientists never needed a crystal to visit...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  content_len  \\\n",
       "0  think fixed either utc standard utc+1 year aro...          178   \n",
       "1  back still listened skrillex songs almost alwa...          182   \n",
       "2  definitive line comes hours anything someone c...          134   \n",
       "3  good audio/video quality including mics banter...          101   \n",
       "4  narrative hl1 lead believe black mesa 's first...          104   \n",
       "\n",
       "                                             summary  summary_len  edit  \\\n",
       "0       shifting seasonal time is no longer worth it            8     1   \n",
       "1  do not listen to dubstep right before you go t...           11     1   \n",
       "2  there is not a definitive skill level it is al...           17     1   \n",
       "3                    good job but more firefall talk            6     1   \n",
       "4  the scientists never needed a crystal to visit...           17     1   \n",
       "\n",
       "   edit_len  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = newdf.toPandas()\n",
    "pdf.drop(['body', 'normalizedBody', 'author', 'id', 'subreddit', \n",
    "          'subreddit_id', 'title'], axis=1, inplace=True)\n",
    "pdf = pdf.replace('', np.NaN)\n",
    "pdf = pdf.replace(float('nan'), np.NaN)\n",
    "pdf.dropna(inplace=True)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267474"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_pickle('df_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = pdf.sample(frac=0.1)\n",
    "# len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pdf.content\n",
    "y = pdf.summary\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import csv     \n",
    "import timeit  \n",
    "import random  \n",
    "\n",
    "\n",
    "\n",
    "default_path = \"./data/\"\n",
    "\n",
    "train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
    "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
    "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
    "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
    "\n",
    "#valid_article_path = default_path + \"sumdata/DUC2003/input.txt\"\n",
    "#valid_title_path   = default_path + \"sumdata/DUC2003/task1_ref0.txt\"\n",
    "\n",
    "train_article_path = X_train\n",
    "train_title_path   = X_test\n",
    "valid_article_path = y_train\n",
    "valid_title_path   = y_test\n",
    "\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def get_text_list(series, toy):\n",
    "    if not toy:\n",
    "        return series.tolist()[:200000]\n",
    "    else:\n",
    "        return series.tolist()[:50]\n",
    "\n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "        train_article_list = get_text_list(train_article_path, toy)\n",
    "        train_title_list = get_text_list(train_title_path, toy)\n",
    "\n",
    "        words = list()\n",
    "        for sentence in train_article_list + train_title_list:\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, _ in word_counter:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 50\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "        title_list = get_text_list(train_title_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in title_list]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(word_dict , reversed_dict, embedding_size):\n",
    "    print(\"Loading Lists...\")\n",
    "    train_article_list = get_text_list(train_article_path, False)\n",
    "    train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "    print(\"Loading TF-IDF...\")\n",
    "    tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "    \n",
    "    print(\"Loading Pos Tags...\")\n",
    "    pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())\n",
    "\n",
    "    #print(\"Loading Named Entity...\")\n",
    "    #named_entity_recs = named_entity(postags_for_named_entity) \n",
    "    \n",
    "    print(\"Loading Glove vectors...\")\n",
    "\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "    with s3.open(\n",
    "        's3://aws-logs-816063959671-us-east-1/data/model_glove_300.pkl', \n",
    "        'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)\n",
    "     \n",
    "    used_words = 0\n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "            if word in tf_idf_list:\n",
    "                v= tf_idf_list[word]\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "\n",
    "            if word in pos_list:\n",
    "                v=pos_list[word]\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2) \n",
    "\n",
    "            #if word in named_entity_recs:\n",
    "            #  v=named_entity_recs[word]\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)\n",
    "            #else:\n",
    "            #  v=0\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)  \n",
    "          \n",
    "            used_words += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32) #to generate for <padding> and <unk>\n",
    "        \n",
    "        \n",
    "        word_vec_list.append(np.array(word_vec))\n",
    "\n",
    "    print(\"words found in glove percentage = \" + str((used_words/len(word_vec_list))*100) )\n",
    "          \n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)\n",
    "\n",
    "def tf_idf_generate(sentences):\n",
    "    #https://stackoverflow.com/questions/30976120/find-the-tf-idf-score-of-specific-words-in-documents-using-sklearn\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    # our corpus\n",
    "    data = sentences\n",
    "    cv = CountVectorizer()\n",
    "    # convert text data into term-frequency matrix\n",
    "    data = cv.fit_transform(data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    # convert term-frequency matrix into tf-idf\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "    # create dictionary to find a tfidf word each word\n",
    "    word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "    return word2tfidf\n",
    "\n",
    "def get_pos_tags_dict(words):\n",
    "    #sent = nltk.word_tokenize(sent)\n",
    "    #print(sent)\n",
    "    post_tags_for_words = nltk.pos_tag(words)\n",
    "    pos_list ={}\n",
    "    for word,pos in post_tags_for_words:\n",
    "        pos_list[word] = pos\n",
    "\n",
    "    df = pd.DataFrame(list(pos_list.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    \n",
    "    pos_list ={}\n",
    "    for index, row in df.iterrows():\n",
    "        pos_list[row['word']] = row['code']\n",
    "    return pos_list , post_tags_for_words\n",
    "\n",
    "def named_entity(post_tags_for_words):\n",
    "    names = ne_chunk(post_tags_for_words)\n",
    "    names_dict = {}\n",
    "    for n in names:\n",
    "        if (len(n) == 1):\n",
    "            named_entity = str(n).split(' ')[0][1:]\n",
    "            word = str(n).split(' ')[1].split('/')[0]\n",
    "            names_dict[word] = named_entity\n",
    "\n",
    "    df = pd.DataFrame(list(names_dict.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "\n",
    "    names_dict ={}\n",
    "    for index, row in df.iterrows():\n",
    "        names_dict[row['word']] = row['code']\n",
    "    print(names_dict)\n",
    "    return names_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loading training dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.08174166850836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading Lists...\")\n",
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "print(\"Loading TF-IDF...\")\n",
    "tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "tf_idf_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529\n",
      "1109\n",
      "529\n"
     ]
    }
   ],
   "source": [
    "print(word_dict[\"apple\"])\n",
    "print(word_dict[\"cat\"])\n",
    "print(word_dict[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "one\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "print(reversed_dict[9])\n",
    "print(reversed_dict[7])\n",
    "print(reversed_dict[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_max_len : 50\n",
      "summary_max_len : 15\n"
     ]
    }
   ],
   "source": [
    "print(\"article_max_len : \" + str(article_max_len))\n",
    "print(\"summary_max_len : \" + str(summary_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 332, 705, 52, 37, 54, 23, 128, 527, 1184, 332, 86, 78, 265, 445, 581, 9873, 112, 65, 1142, 626, 13340, 154, 284, 2143, 363, 74, 1219, 34108, 20172, 581, 306, 3724, 3947, 478, 120, 284, 527, 53, 8, 84, 265, 5, 45, 100, 22, 19, 581, 135, 34]\n",
      "dating\n",
      "boyfriend\n",
      "nearly\n",
      "2\n",
      "years\n",
      "pretty\n",
      "good\n",
      "together\n",
      "talked\n",
      "marriage\n",
      "boyfriend\n",
      "great\n",
      "guy\n",
      "seem\n",
      "interested\n",
      "perfect\n",
      "upsets\n",
      "everything\n",
      "anything\n",
      "asks\n",
      "constantly\n",
      "obsess\n",
      "making\n",
      "happy\n",
      "ca\n",
      "n't\n",
      "help\n",
      "natural\n",
      "pleaser\n",
      "molded\n",
      "perfect\n",
      "girlfriend\n",
      "hobbies\n",
      "sacrifice\n",
      "mine\n",
      "keep\n",
      "happy\n",
      "talked\n",
      "every\n",
      "time\n",
      "try\n",
      "seem\n",
      "like\n",
      "new\n",
      "end\n",
      "going\n",
      "back\n",
      "perfect\n",
      "girl\n",
      "feel\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "for num in train_x[0] :\n",
    "    print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2162, 7, 1291, 97, 896, 504, 1782, 105, 478, 407, 766, 340, 21, 12530]\n",
      "owned\n",
      "one\n",
      "six\n",
      "months\n",
      "truly\n",
      "amazing\n",
      "animals\n",
      "however\n",
      "mine\n",
      "turned\n",
      "trouble\n",
      "worth\n",
      "got\n",
      "breeder\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])\n",
    "for num in train_y[0] :\n",
    "    print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n",
      "Loading Pos Tags...\n",
      "Loading Glove vectors...\n",
      "words found in glove percentage = 28.77891899751092\n"
     ]
    }
   ],
   "source": [
    "test_embedding = get_init_embedding(word_dict , reversed_dict, 320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embedding[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460813, 320)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = get_pos_tags_dict(word_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from utils import get_init_embedding\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove: #training\n",
    "                #init_embeddings = tf.constant(get_init_embedding(word_dict ,reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "                init_embeddings = tf.constant(test_embedding, dtype=tf.float32)\n",
    "            else: #testing\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "            \n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only: #trainig\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "                \n",
    "            else: #testing\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only: #training\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/saved_model_2/checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-98bfa2b96024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#if args.with_model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mold_model_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'saved_model_2/checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mold_model_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"saved_model_2/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_model_checkpoint_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/saved_model_2/checkpoint'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "#from model import Model\n",
    "#from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
    "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#def add_arguments(parser):\n",
    "#    parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
    "#    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
    "#    parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
    "#    parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
    "#    parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
    "#\n",
    "#    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
    "#    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
    "#    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of epochs.\")\n",
    "#    parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
    "#\n",
    "#    parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
    "#\n",
    "#    parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#add_arguments(parser)\n",
    "#args = parser.parse_args()\n",
    "#with open(\"args.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(args, f)\n",
    "\n",
    "if not os.path.exists(default_path + \"saved_model_2\"):\n",
    "    os.mkdir(default_path + \"saved_model_2\")\n",
    "else:\n",
    "    #if args.with_model:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model_2/checkpoint', 'r')\n",
    "        old_model_checkpoint_path = \"\".join([default_path + \"saved_model_2/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    for batch_x, batch_y in batches:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model_2/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model...\n",
      "WARNING:tensorflow:From /home/hadoop/anaconda/lib/python3.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:733: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model_checkpoint_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4e36635a4c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"saved_model_2/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model_checkpoint_path'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Loading dictionary...\")\n",
    "#word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "#print(\"Loading validation dataset...\")\n",
    "#valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "#valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "#print(\"Loading article and reference...\")\n",
    "#article = get_text_list(valid_article_path, args.toy)\n",
    "#reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model_2/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'result2.txt'...\")\n",
    "    \n",
    "    for batch_x, _ in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        with open(default_path + \"model_2_files/result2.txt\", \"wb\") as f:\n",
    "            for line in prediction_output:\n",
    "                summary = list()\n",
    "                for word in line:\n",
    "                    if word == \"</s>\":\n",
    "                        break\n",
    "                    if word not in summary:\n",
    "                        summary.append(word)\n",
    "                summary_array.append(\" \".join(summary))\n",
    "                #print(\" \".join(summary), file=f)\n",
    "\n",
    "    print('Summaries have been generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/model_2_files/result2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3126157940e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msummary_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model_2_files/result2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/model_2_files/result2.txt'"
     ]
    }
   ],
   "source": [
    "summary_array = []\n",
    "with open(default_path + \"model_2_files/result2.txt\", \"wb\") as f:\n",
    "    for line in prediction_output:\n",
    "        summary = list()\n",
    "        for word in line:\n",
    "            if word == \"</s>\":\n",
    "                break\n",
    "            if word not in summary:\n",
    "                summary.append(word)\n",
    "        summary_array.append(\" \".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_array[:2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
