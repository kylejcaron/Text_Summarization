{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions take from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "CONTRACTIONS = { \n",
    "\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\"i'll\": \"i will\",\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, FloatType, DoubleType, LongType\n",
    "from pyspark.sql import Row\n",
    "import pickle\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-13581a336be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = spark.read.json(\n\u001b[0m\u001b[1;32m      2\u001b[0m     's3://aws-logs-816063959671-us-east-1/data/tldr-training-data.jsonl')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\n",
    "    's3://aws-logs-816063959671-us-east-1/data/tldr-training-data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a subset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.sample(withReplacement=False, fraction=0.1)\n",
    "subset.cache()\n",
    "subset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('body','content', 'id', 'normalizedBody', 'subreddit', 'title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select([count(when(isnan(c), c)).alias(c) for c in subset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in subset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}% of the subset is missing reddit post titles'.format(\n",
    "    round(subset.filter(subset[\"title\"].isNull()).count()/subset.count()*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few entries have null subreddits. Investigate further\n",
    "\n",
    "#### Also, explore the difference between body, content, and normalized body\n",
    "\n",
    "Going to look at each of these entries where subreddit is null and make sure nothing is wrong with them, as well as use this to understand the differences between textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_null = subset.filter(subset[\"subreddit\"].isNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('index: ' + str(i))\n",
    "    print('Body:')\n",
    "    print(subreddit_null.loc[i,'body'])\n",
    "    print('-'*60)\n",
    "    print('Content:')\n",
    "    print(subreddit_null.loc[i,'content'])\n",
    "    print('-'*60)\n",
    "    print('Normalized Body:')\n",
    "    print(subreddit_null.loc[i,'normalizedBody'])\n",
    "    print('-'*60)\n",
    "    print('Summary:')\n",
    "    print(subreddit_null.loc[i,'summary'])\n",
    "    print('-'*60)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all happen to be spam posts, so for cleaning, all rows where subreddit is null should be dropped. \n",
    "\n",
    "In terms of what these textual features are:\n",
    "* __body__ appears to be the original post, with the tl;dr (aka 'too long; didn't read', reddit's term for a summary)\n",
    "* __content__ is a cleaned version of __body__, without the tl;dr\n",
    "* __normalizedBody__ is a cleaned version of __body__ (with the tl;dr)\n",
    "* __summary__ is simply the tl;dr\n",
    "\n",
    "An approach for EDA should be to confirm that content length is the same length as the normalizedBody without the tl;dr/summary. Summary's with less than 2-3 words should be reviewed and potentially dropped, since I don't want to summarize entries with only a 2-3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_subreddit = df.select(\"subreddit\").distinct().count()\n",
    "total_len_subreddit = df.select(\"subreddit\").count()\n",
    "print('There are {} different subreddits, which is roughly {}% of the dataframe.'.format(\n",
    "        nunique_subreddit, (round(nunique_subreddit/total_len_subreddit*100,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in subset.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few null subreddits (which were identified as spam above), which should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out all rows where subreddit is null\n",
    "subset = subset.filter(subset.subreddit.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make all text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['body', 'content', 'normalizedBody', 'subreddit', 'summary', 'title']:\n",
    "    subset = subset.withColumn(col, F.lower(F.col(col)))\n",
    "# Converts 'null' strings in the title column back to null values\n",
    "subset = subset.withColumn('title', when(subset.title == 'null', F.lit(None)).otherwise(subset.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring summary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.describe('summary_len').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these have a min of 1 word, which is far too short (even 2-5 words may be too short), while some have 377 words, which is too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that mony columns contain 'edits' of their original post, but it is in the summary. The best approach is likely to slice off any text after 'edit:', since such a large amount of posts contain this. Will create a new column to contain this edited information in case there is any useful information there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}% of the dataset contains edits'.format(\n",
    "    round(subset[subset['summary'].rlike('(edit:|[^a-z]edit)')].count()/subset.count()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two approaches to take: \n",
    " * remove all entries with 'edit'\n",
    " * split summaries before and after 'edit', and use imputation to evaluate with/without those rows\n",
    " \n",
    "Going to attempt approach 2, which is more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = F.split(subset['summary'], '(edit:|[^a-z]edit)')\n",
    "subset = subset.withColumn('edit', split_col.getItem(1))\n",
    "subset = subset.withColumn('summary', split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readjust summary_len column\n",
    "\n",
    "The summaries are now shorter because 'edit' is gone. summary length now needs to be recalculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_tokenize('go for it, nobody cares. \\n ')))\n",
    "word_tokenize('go for it, nobody cares. \\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's going to be necessary to make a user defined function to properly get word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_length('go for it, nobody cares. \\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = udf(word_length, LongType())\n",
    "subset = subset.withColumn('summary_len_cleaned', function(subset.summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Edit_len column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with '' in edit column so udf does not fail\n",
    "subset = subset.withColumn('edit', subset.edit).na.fill('')\n",
    "# Creates edit_len column, number of words from 'edit'\n",
    "function = udf(word_length, LongType())\n",
    "subset = subset.withColumn('edit_len', function(subset.edit))\n",
    "# Converts -1 in edit_len column to null\n",
    "subset = subset.withColumn('edit_len', \n",
    "        when(subset.edit_len == -1, F.lit(0)).otherwise(subset.edit_len))\n",
    "# Turn's the edit column into a bool/imputation\n",
    "subset = subset.withColumn('edit', when(subset.edit.isNull(), F.lit(0)).otherwise(1))\n",
    "# Count summary's containing 'edit:'\n",
    "subset[subset['summary'].rlike('(edit:|[^a-z]edit )')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary_len','summary_len_cleaned','edit','edit_len').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining summarys with less than 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} summaries with 1 word'.format(subset.filter(subset.summary_len == 1).count()))\n",
    "print('{} summaries with less than 5 words'.format(subset.filter(subset.summary_len < 5).count()))\n",
    "print('{}% of all entries have less than 5 words'.format(\n",
    "    round(subset.filter(subset.summary_len < 5).count()/subset.count()*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').filter(subset.summary_len == 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').filter((subset.summary_len == 4)).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of 4 is getting better, but still too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').filter((subset.summary_len == 5)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of 5 words is a good minimum cut off for cleaning, especially since this is only 5% of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code removes all rows where the summary contains less than 5 words\n",
    "subset = subset.filter(subset.summary_len >= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining summarys with too many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the largest word count summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary_len').orderBy(F.desc(\"summary_len\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').orderBy(F.desc(\"summary_len_cleaned\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly not the sort of summary we are hoping to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for i in range(0, 100, 10):\n",
    "    counts.append(subset.select('summary').filter((subset.summary_len_cleaned >= i)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[0,10,20,30,40,50,60, 70, 80, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_n50 = round(subset.select('summary').filter((subset.summary_len >= 50)).count()/subset.count()*100,2)\n",
    "percent_n100 = round(subset.select('summary').filter((subset.summary_len >= 100)).count()/subset.count()*100,2)\n",
    "\n",
    "print('50-word cutoff: {}% of the data\\n 100-word cutoff: {}% of the data'.format(percent_n50, percent_n100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').filter((subset.summary_len == 100)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary').filter((subset.summary_len == 50)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Words may be a good cutoff but still could be too many words. it is getting more difficult to quantify. Maybe a measure of the summary length containing x% of the body length would be a good additional measure.\n",
    "\n",
    "In the meantime, in the model.py file, the cleaning function will include a variable to specify the length of summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.filter((subset.summary_len_cleaned <= 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out summaries that have too close of a word length to the content\n",
    "\n",
    "We are looking to create concise summaries from larger content. A 50% word cut-off may be a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('summary_len_cleaned', 'content_len').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('content_len', 'summary_len_cleaned').filter(\n",
    "    (subset.summary_len_cleaned <= subset.content_len*0.5)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('content_len', 'summary_len_cleaned').filter((\n",
    "    subset.summary_len_cleaned <= subset.content_len*0.5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes all rows where the summary length is not less than 50% of the content length\n",
    "subset = subset.filter(subset.summary_len_cleaned <= subset.content_len*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to clean the Reddit Post Content, not the summary, since we want the output of our model to be plain english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = [CONTRACTIONS[w] if w in CONTRACTIONS else w for w in text]\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    if remove_stopwords==True:\n",
    "        tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text('testing it out', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext_udf = udf(clean_text, StringType())\n",
    "subset = subset.withColumn('content_cleaned', cleantext_udf(subset.content, F.lit(True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.select('content_cleaned').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the dataset is now cleaned and ready for analysis. src/util.py contains a function to automate exactly what was accomplished in this notebook, and is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    text = text.lower()\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = [CONTRACTIONS[w] if w in CONTRACTIONS else w for w in text]\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    if remove_stopwords==True:\n",
    "        tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "def word_length(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = [w for w in tokens if w not in PUNCTUATION]\n",
    "    return len(tokens)\n",
    "\n",
    "def clean_data(df, n_words_summary=50, remove_stopwords=True):\n",
    "    # Get rid of all rows where subreddit is null (these are spam)\n",
    "    df = df.filter(df.subreddit.isNotNull())\n",
    "    # Lowercase columns:\n",
    "    for col in ['body','content','normalizedBody','subreddit','summary','title']:\n",
    "        df = df.withColumn(col, F.lower(F.col(col)))\n",
    "    # Converts 'null' strings in the title column back to null values\n",
    "    df = df.withColumn('title', when(df.title == 'null', F.lit(None)).otherwise(df.title))\n",
    "\n",
    "    # Creat edit(bool) and edit_len columns, while removing 'edit:%' from summary column\n",
    "    split_col = F.split(df['summary'], '(edit:|[^a-z]edit)')\n",
    "    df = df.withColumn('edit', split_col.getItem(1))\n",
    "    df = df.withColumn('summary', split_col.getItem(0))\n",
    "    function = udf(word_length, LongType())\n",
    "    df = df.withColumn('summary_len', function(df.summary))\n",
    "        # Creates edit_len column, number of words from 'edit'\n",
    "    df = df.withColumn('edit', df.edit).na.fill('')\n",
    "    df = df.withColumn('edit_len', function(df.edit))\n",
    "        # Converts -1 in edit_len column to null\n",
    "    df = df.withColumn('edit_len',\n",
    "        when(df.edit_len == -1, F.lit(0)).otherwise(df.edit_len))\n",
    "    df = df.withColumn('edit', when(df.edit.isNull(), F.lit(0)).otherwise(1))\n",
    "    # Remove all rows where summary contains less than 5 words\n",
    "    df = df.filter(df.summary_len >= 5)\n",
    "    # Remove all rows where summary contains greater than n_words_summary words\n",
    "    df = df.filter((df.summary_len <= n_words_summary))\n",
    "    # Remove all rows where the summary length is not less than 50% of the content length\n",
    "    df = df.filter(df.summary_len <= df.content_len*0.5)\n",
    "    # Clean Content column\n",
    "    cleantext_udf = udf(clean_text, StringType())\n",
    "    df = df.withColumn('content', cleantext_udf(df.content, F.lit(remove_stopwords)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(subset).take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
